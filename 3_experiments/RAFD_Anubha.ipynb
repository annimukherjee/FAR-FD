{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vLGyxxRwcNFq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import faiss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCdTUn0Zbt7O",
        "outputId": "319cd6c1-35bb-49e0-d242-9ba54812b565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(118108, 32)\n"
          ]
        }
      ],
      "source": [
        "# loading the entire test embeddings\n",
        "path = \"./cls_embeddings_time.struct_time(tm_year=2025, tm_mon=2, tm_mday=7, tm_hour=19, tm_min=40, tm_sec=22, tm_wday=4, tm_yday=38, tm_isdst=0).npy\"\n",
        "\n",
        "\n",
        "cls_embeddings = np.load(path)\n",
        "print(cls_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cls</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card4</th>\n",
              "      <th>card6</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>...</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.291883</td>\n",
              "      <td>-0.329939</td>\n",
              "      <td>0.108390</td>\n",
              "      <td>-0.145421</td>\n",
              "      <td>-0.399322</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.892993</td>\n",
              "      <td>0.871243</td>\n",
              "      <td>-0.359702</td>\n",
              "      <td>0.680504</td>\n",
              "      <td>-0.412094</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.030054</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.594876</td>\n",
              "      <td>-1.467121</td>\n",
              "      <td>8.134522</td>\n",
              "      <td>-0.109308</td>\n",
              "      <td>0.711822</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.123148</td>\n",
              "      <td>-0.156138</td>\n",
              "      <td>-0.422421</td>\n",
              "      <td>1.487250</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>0.341765</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.611964</td>\n",
              "      <td>1.677853</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>-0.081355</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76765</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>-1.327921</td>\n",
              "      <td>-1.260784</td>\n",
              "      <td>-0.113217</td>\n",
              "      <td>-0.653259</td>\n",
              "      <td>-1.606254</td>\n",
              "      <td>...</td>\n",
              "      <td>1.009878</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76766</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>0.675641</td>\n",
              "      <td>0.648266</td>\n",
              "      <td>-0.075376</td>\n",
              "      <td>-0.002802</td>\n",
              "      <td>0.756523</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76767</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0.418154</td>\n",
              "      <td>0.377752</td>\n",
              "      <td>0.150412</td>\n",
              "      <td>-1.485918</td>\n",
              "      <td>-0.226903</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76768</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>0.605578</td>\n",
              "      <td>0.576883</td>\n",
              "      <td>-0.322279</td>\n",
              "      <td>-0.182963</td>\n",
              "      <td>0.577719</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76769</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>0.944849</td>\n",
              "      <td>0.929392</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>1.061025</td>\n",
              "      <td>1.229079</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76770 rows × 182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       cls  ProductCD  card4  card6  P_emaildomain  Unnamed: 0  TransactionDT  \\\n",
              "0        0          4      2      1              2   -0.291883      -0.329939   \n",
              "1        0          4      3      2             16    0.892993       0.871243   \n",
              "2        0          4      2      2              1   -1.594876      -1.467121   \n",
              "3        0          4      3      2             19   -0.123148      -0.156138   \n",
              "4        0          4      3      2             16    1.611964       1.677853   \n",
              "...    ...        ...    ...    ...            ...         ...            ...   \n",
              "76765    0          4      3      2             16   -1.327921      -1.260784   \n",
              "76766    0          4      2      2             25    0.675641       0.648266   \n",
              "76767    0          4      1      1             16    0.418154       0.377752   \n",
              "76768    0          4      3      2             19    0.605578       0.576883   \n",
              "76769    0          4      3      2             53    0.944849       0.929392   \n",
              "\n",
              "       TransactionAmt     card1     card2  ...      V312      V313      V314  \\\n",
              "0            0.108390 -0.145421 -0.399322  ... -0.227583 -0.222385 -0.249222   \n",
              "1           -0.359702  0.680504 -0.412094  ... -0.030054 -0.222385 -0.249222   \n",
              "2            8.134522 -0.109308  0.711822  ... -0.227583 -0.222385 -0.249222   \n",
              "3           -0.422421  1.487250 -0.265218  ...  0.341765 -0.222385 -0.249222   \n",
              "4           -0.317889 -0.081355 -0.265218  ... -0.227583 -0.222385 -0.249222   \n",
              "...               ...       ...       ...  ...       ...       ...       ...   \n",
              "76765       -0.113217 -0.653259 -1.606254  ...  1.009878 -0.222385 -0.249222   \n",
              "76766       -0.075376 -0.002802  0.756523  ... -0.227583 -0.222385 -0.249222   \n",
              "76767        0.150412 -1.485918 -0.226903  ... -0.227583  0.393449  0.090945   \n",
              "76768       -0.322279 -0.182963  0.577719  ... -0.227583 -0.222385 -0.249222   \n",
              "76769       -0.317889  1.061025  1.229079  ... -0.227583  0.393449  0.090945   \n",
              "\n",
              "           V315      V316      V317      V318      V319      V320      V321  \n",
              "0     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "1     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "2     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "3     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "4     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "76765 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76766 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76767  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76768 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76769  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "\n",
              "[76770 rows x 182 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"./data/train_df.csv\")\n",
        "train_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76770, 1)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y = pd.read_csv(\"./data/train_y_df.csv\")\n",
        "train_y['isFraud'] = train_y['isFraud'].astype(np.float32)\n",
        "train_y.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(23622, 182)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv(\"./data/test_df.csv\")\n",
        "test_df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_y_df = pd.read_csv(\"./data/test_y_df.csv\")\n",
        "test_y_df['isFraud'] = test_y_df['isFraud'].astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        0.0\n",
              "1        0.0\n",
              "2        0.0\n",
              "3        0.0\n",
              "4        0.0\n",
              "        ... \n",
              "23617    0.0\n",
              "23618    0.0\n",
              "23619    1.0\n",
              "23620    0.0\n",
              "23621    0.0\n",
              "Name: isFraud, Length: 23622, dtype: float32"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_y_df['isFraud']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76770, 1)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### splitting embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute 65% of the total rows\n",
        "total_rows = cls_embeddings.shape[0]\n",
        "train_size = int(0.65 * total_rows)  # 65% of 118108\n",
        "\n",
        "# Slice the top 65%\n",
        "train_embeddings = cls_embeddings[:train_size]  # First 65%\n",
        "\n",
        "# print(f\"Total embed shape: {cls_embeddings.shape}\")\n",
        "# print(f\"Train embed shape: {train_embeddings.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_rows = cls_embeddings.shape[0]\n",
        "test_size = int(0.8 * total_rows) \n",
        "\n",
        "test_embeddings = cls_embeddings[test_size:]\n",
        "# print(f\"Total embed shape: {cls_embeddings.shape}\")\n",
        "# print(f\"Train embed shape: {test_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### faiss index and similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA6ULff8bxV-",
        "outputId": "33b81774-3e8c-48c4-cd3a-f81370fc6775"
      },
      "outputs": [],
      "source": [
        "def create_index(num_embeddings, dimension):\n",
        "\n",
        "\n",
        "    # num_embeddings = 76770\n",
        "    # dimension = 32\n",
        "\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 similarity\n",
        "\n",
        "    index.add(train_embeddings)  # index of pre-computed embeddings\n",
        "\n",
        "    # k = 120  # as best result for 120\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "# query_vector = np.random.random((1, dimension)).astype(\"float32\") #random for now\n",
        "\n",
        "\n",
        "# print(k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_faiss(query_vector):\n",
        "\n",
        "    index = create_index(76770, 32)\n",
        "\n",
        "    # Convert PyTorch tensor to NumPy\n",
        "    if isinstance(query_vector, torch.Tensor):\n",
        "        query_vector = query_vector.detach().cpu().numpy()\n",
        "\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "\n",
        "    distances, indices = index.search(query_vector, k=120) # by default using euclidean distance for similarity\n",
        "    indices = indices.flatten()\n",
        "\n",
        "\n",
        "    return distances, indices\n",
        "\n",
        "# distances, indices = index.search(\n",
        "#     query_vector, k\n",
        "# )  \n",
        "\n",
        "# print(\"Input Sample embedding:\", query_vector)\n",
        "# print(\"Indices of nearest neighbors:\", indices)\n",
        "# print(\"L2 norm distances\", distances)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### L2 distance component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WJLNSJdcbznQ"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(distances, dropout=0.2):\n",
        "\n",
        "    # print(f\"distances.shape before: {distances.shape}\")\n",
        "    distances = distances.flatten()\n",
        "    # print(f\"distances.shape after: {distances.shape}\")\n",
        "\n",
        "    # Apply softmax to the negative distances\n",
        "    similarities = np.exp(-distances)\n",
        "    softmax_scores = similarities / np.sum(similarities)\n",
        "\n",
        "    # Apply dropout (randomly zero out some softmax scores)\n",
        "    dropout_mask = np.random.binomial(1, 1 - dropout, size=softmax_scores.shape)\n",
        "    dropped_softmax_scores = softmax_scores * dropout_mask\n",
        "\n",
        "    final_softmax = dropped_softmax_scores / np.sum(dropped_softmax_scores)\n",
        "    # how to weigh in the final embedding?\n",
        "    return final_softmax, distances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### mask to drop the dropped out values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_by_mask(arr1, arr2, arr3):\n",
        "    \"\"\"\n",
        "    Removes elements from arr2 and arr3 where corresponding indices in arr1 are zero.\n",
        "    \"\"\"\n",
        "    mask = arr1 != 0  # Create a boolean mask where arr1 is nonzero\n",
        "    return arr1[mask], arr2[mask], arr3[mask]\n",
        "\n",
        "# S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "\n",
        "# print(\"Filtered S_x_xi:\", S_x_xi)\n",
        "# print(\"Filtered indices:\", indices)\n",
        "# print(\"Filtered distances:\", distances)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### linear trasnform the value component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlB6RGt6cLid",
        "outputId": "27a8197e-bb5c-4bd2-f934-6fcc9eb8cb6f"
      },
      "outputs": [],
      "source": [
        "class MLP_L1(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_L1, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.activation1 = nn.SiLU()\n",
        "\n",
        "        self.layer2 = nn.Linear(32, 32)\n",
        "        self.activation2 = nn.SiLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.layer3 = nn.Linear(32, 32)  # Fix: Output should be 32\n",
        "        self.activation3 = nn.SiLU()  # Fix: Apply SiLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.layer1(x))\n",
        "        x = self.dropout2(self.activation2(self.layer2(x)))\n",
        "        x = self.activation3(self.layer3(x))  # Fix: Apply activation & dropout\n",
        "        return x\n",
        "\n",
        "\n",
        "# # Create model instance\n",
        "# model = MLP_L1(l1_dist.shape[0])\n",
        "\n",
        "# # Convert input to tensor and pass it through the model\n",
        "# l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
        "\n",
        "# w_v_l1 = model(l1_dist_tensor).detach().numpy()\n",
        "# print(w_v_l1)\n",
        "\n",
        "# print(w_v_l1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXAE9Vzwcn2T",
        "outputId": "5541a00b-ec38-4c32-e4c2-0f6f8998f8fc"
      },
      "outputs": [],
      "source": [
        "def compute_l1(distances):\n",
        "    l1_dist = np.sqrt(distances)\n",
        "    # print(l1_dist, l1_dist.shape)\n",
        "\n",
        "    model = MLP_L1(l1_dist.shape[0])\n",
        "\n",
        "    # Convert input to tensor and pass it through the model\n",
        "    l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
        "\n",
        "    w_v_l1 = model(l1_dist_tensor).detach().numpy()\n",
        "    # print(w_v_l1)\n",
        "\n",
        "    # print(w_v_l1.shape)\n",
        "\n",
        "    return w_v_l1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modified MLP to output shape (60,)\n",
        "class MLP_Wy(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_Wy, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # First layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 32)  # Output layer (1 neuron)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)  # Shape (60, 1)\n",
        "        return x.squeeze(1)  # Shape (60,)\n",
        "\n",
        "# Instantiate MLP with input_dim=32 (from Wy)\n",
        " # Expected: (60,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76765</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76766</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76767</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76768</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76769</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76770 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       isFraud\n",
              "0          0.0\n",
              "1          0.0\n",
              "2          0.0\n",
              "3          0.0\n",
              "4          0.0\n",
              "...        ...\n",
              "76765      0.0\n",
              "76766      0.0\n",
              "76767      0.0\n",
              "76768      1.0\n",
              "76769      0.0\n",
              "\n",
              "[76770 rows x 1 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_wy(indices):\n",
        "    y_i = train_y['isFraud'].loc[indices].values\n",
        "    # print(f\"y_i.shape: {y_i.shape}\")\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "    feature_target_tensor = torch.tensor(y_i, dtype=torch.long)\n",
        "\n",
        "# Define Wy: An embedding layer to map to 32-dim space\n",
        "    embedding_dim = 32\n",
        "    num_classes = 2  # Since input values are 0 or 1\n",
        "\n",
        "    Wy = nn.Embedding(num_classes, embedding_dim)\n",
        "\n",
        "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
        "\n",
        "# Compute embeddings using Wy\n",
        "    embeddings = Wy(feature_target_tensor)  # Shape: (60, 32)\n",
        "\n",
        "    # Pass embeddings through MLP\n",
        "    w_y = mlp(embeddings)  # Shape: (60,)\n",
        "\n",
        "    # print(\"MLP Output Shape:\", w_y.shape) \n",
        "\n",
        "    return w_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_value(w_v_l1, w_y):\n",
        "    # Compute the dot product of w_v_l1 and w_y\n",
        "    w_y_npy = w_y.detach().numpy()\n",
        "    value = w_y_npy + w_v_l1\n",
        "    return value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reshaping S to do S * V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_z_in(S_x_xi, value):\n",
        "    S_x_xi = S_x_xi.reshape(1, -1)\n",
        "    # print(S_x_xi.shape)  # Output should be (1, 95)\n",
        "\n",
        "    # result = (S_x_xi @ value)\n",
        "\n",
        "    # # Summation over all elements (since it's 1D)\n",
        "    # Z = np.sum(result)\n",
        "\n",
        "    # Assuming S_x_xi is (1, 95) and value is (95, 32)\n",
        "    numerator = np.sum(S_x_xi @ value)  # Sum the weighted contributions (scalar)\n",
        "    denominator = np.sum(S_x_xi)        # Total sum of weights (scalar)\n",
        "    z_in = numerator / denominator         # Weighted average as a single scalar\n",
        "\n",
        "    # print(\"Z:\", z_in)\n",
        "    # print(\"Z Shape:\", z_in.shape)  # Expected: torch.Size([])\n",
        "\n",
        "\n",
        "    return z_in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(182,)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test = test_df.iloc[44].values\n",
        "x_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_emb_dim, hidden_dim=32, dropout_prob=0.2):\n",
        "        super(Predictor, self).__init__()\n",
        "        \n",
        "        self.input_dim = input_emb_dim + 1  # Adding 2 for weighted_avg and f_z_in\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.LayerNorm(self.input_dim),\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_emb, weighted_avg):\n",
        "        # Ensure correct shape for scalar inputs\n",
        "        weighted_avg = weighted_avg.unsqueeze(-1)  \n",
        "    \n",
        "        # Concatenate all inputs\n",
        "        combined = torch.cat([input_emb, weighted_avg], dim=-1)\n",
        "\n",
        "        # Pass through MLP blocks\n",
        "        x = self.block1(combined)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def process_samples(test_df, test_embeddings):\n",
        "#     test_i = test_df.iloc[i].values #shape (182,)\n",
        "#     query_vector = test_embeddings[i] #shape (32,)\n",
        "#     distances, indices = search_faiss(query_vector) # both shape (120,) and flatten\n",
        "#     S_x_xi, distances = compute_similarity(distances)\n",
        "#     S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "#     w_v_l1 = compute_l1(distances) #shape (32,)\n",
        "#     w_y = compute_wy(indices) #shape (32,)\n",
        "#     value = compute_value(w_v_l1, w_y)\n",
        "#     z_in = compute_z_in(S_x_xi, value)\n",
        "\n",
        "#     input_to_mlp = test_i * z_in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_samples(test_df, test_embeddings):\n",
        "    \"\"\"Processes all samples and returns input tensor and labels.\"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    for i in range(len(test_df)):  # Process all samples\n",
        "        test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "        query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "        distances, indices = search_faiss(query_embedding)\n",
        "        S_x_xi, distances = compute_similarity(distances)\n",
        "        S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "        w_v_l1 = compute_l1(distances)  # shape (32,)\n",
        "        w_y = compute_wy(indices)  # shape (32,)\n",
        "        value = compute_value(w_v_l1, w_y)\n",
        "        z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
        "\n",
        "\n",
        "        # Append only test_i (input_emb) and z_in (weighted_avg), not query_embedding\n",
        "        z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "\n",
        "        input_list.append((test_i, z_in))\n",
        "\n",
        "        labels.append(torch.tensor(test_y_df.iloc[i]['isFraud'], dtype=torch.float32))  # Assuming label is in df\n",
        "\n",
        "\n",
        "\n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_list, labels = process_samples(test_df, test_embeddings )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 0.3888, AUCPR: 0.0361\n",
            "Epoch [2/100], Loss: 0.2066, AUCPR: 0.0441\n",
            "Epoch [3/100], Loss: 0.1547, AUCPR: 0.1021\n",
            "Epoch [4/100], Loss: 0.1433, AUCPR: 0.1534\n",
            "Epoch [5/100], Loss: 0.1403, AUCPR: 0.1834\n",
            "Epoch [6/100], Loss: 0.1380, AUCPR: 0.1873\n",
            "Epoch [7/100], Loss: 0.1357, AUCPR: 0.2069\n",
            "Epoch [8/100], Loss: 0.1331, AUCPR: 0.2253\n",
            "Epoch [9/100], Loss: 0.1312, AUCPR: 0.2388\n",
            "Epoch [10/100], Loss: 0.1306, AUCPR: 0.2503\n",
            "Epoch [11/100], Loss: 0.1277, AUCPR: 0.2692\n",
            "Epoch [12/100], Loss: 0.1279, AUCPR: 0.2615\n",
            "Epoch [13/100], Loss: 0.1280, AUCPR: 0.2605\n",
            "Epoch [14/100], Loss: 0.1284, AUCPR: 0.2740\n",
            "Epoch [15/100], Loss: 0.1260, AUCPR: 0.2780\n",
            "Epoch [16/100], Loss: 0.1255, AUCPR: 0.2837\n",
            "Epoch [17/100], Loss: 0.1254, AUCPR: 0.2898\n",
            "Epoch [18/100], Loss: 0.1239, AUCPR: 0.2911\n",
            "Epoch [19/100], Loss: 0.1250, AUCPR: 0.2947\n",
            "Epoch [20/100], Loss: 0.1246, AUCPR: 0.2893\n",
            "Epoch [21/100], Loss: 0.1235, AUCPR: 0.3002\n",
            "Epoch [22/100], Loss: 0.1227, AUCPR: 0.3007\n",
            "Epoch [23/100], Loss: 0.1231, AUCPR: 0.2976\n",
            "Epoch [24/100], Loss: 0.1227, AUCPR: 0.2992\n",
            "Epoch [25/100], Loss: 0.1228, AUCPR: 0.3066\n",
            "Epoch [26/100], Loss: 0.1242, AUCPR: 0.3019\n",
            "Epoch [27/100], Loss: 0.1224, AUCPR: 0.3025\n",
            "Epoch [28/100], Loss: 0.1202, AUCPR: 0.3175\n",
            "Epoch [29/100], Loss: 0.1209, AUCPR: 0.3093\n",
            "Epoch [30/100], Loss: 0.1203, AUCPR: 0.3079\n",
            "Epoch [31/100], Loss: 0.1220, AUCPR: 0.3041\n",
            "Epoch [32/100], Loss: 0.1194, AUCPR: 0.3274\n",
            "Epoch [33/100], Loss: 0.1191, AUCPR: 0.3273\n",
            "Epoch [34/100], Loss: 0.1212, AUCPR: 0.3246\n",
            "Epoch [35/100], Loss: 0.1200, AUCPR: 0.3179\n",
            "Epoch [36/100], Loss: 0.1202, AUCPR: 0.3269\n",
            "Epoch [37/100], Loss: 0.1195, AUCPR: 0.3282\n",
            "Epoch [38/100], Loss: 0.1208, AUCPR: 0.3140\n",
            "Epoch [39/100], Loss: 0.1190, AUCPR: 0.3238\n",
            "Epoch [40/100], Loss: 0.1178, AUCPR: 0.3369\n",
            "Epoch [41/100], Loss: 0.1196, AUCPR: 0.3324\n",
            "Epoch [42/100], Loss: 0.1179, AUCPR: 0.3385\n",
            "Epoch [43/100], Loss: 0.1173, AUCPR: 0.3426\n",
            "Epoch [44/100], Loss: 0.1184, AUCPR: 0.3363\n",
            "Epoch [45/100], Loss: 0.1172, AUCPR: 0.3451\n",
            "Epoch [46/100], Loss: 0.1170, AUCPR: 0.3340\n",
            "Epoch [47/100], Loss: 0.1186, AUCPR: 0.3336\n",
            "Epoch [48/100], Loss: 0.1181, AUCPR: 0.3332\n",
            "Epoch [49/100], Loss: 0.1169, AUCPR: 0.3390\n",
            "Epoch [50/100], Loss: 0.1175, AUCPR: 0.3445\n",
            "Epoch [51/100], Loss: 0.1169, AUCPR: 0.3469\n",
            "Epoch [52/100], Loss: 0.1161, AUCPR: 0.3500\n",
            "Epoch [53/100], Loss: 0.1159, AUCPR: 0.3451\n",
            "Epoch [54/100], Loss: 0.1177, AUCPR: 0.3501\n",
            "Epoch [55/100], Loss: 0.1170, AUCPR: 0.3448\n",
            "Epoch [56/100], Loss: 0.1152, AUCPR: 0.3532\n",
            "Epoch [57/100], Loss: 0.1165, AUCPR: 0.3521\n",
            "Epoch [58/100], Loss: 0.1163, AUCPR: 0.3478\n",
            "Epoch [59/100], Loss: 0.1146, AUCPR: 0.3550\n",
            "Epoch [60/100], Loss: 0.1161, AUCPR: 0.3491\n",
            "Epoch [61/100], Loss: 0.1160, AUCPR: 0.3502\n",
            "Epoch [62/100], Loss: 0.1158, AUCPR: 0.3502\n",
            "Epoch [63/100], Loss: 0.1143, AUCPR: 0.3592\n",
            "Epoch [64/100], Loss: 0.1140, AUCPR: 0.3634\n",
            "Epoch [65/100], Loss: 0.1138, AUCPR: 0.3666\n",
            "Epoch [66/100], Loss: 0.1161, AUCPR: 0.3527\n",
            "Epoch [67/100], Loss: 0.1133, AUCPR: 0.3745\n",
            "Epoch [68/100], Loss: 0.1141, AUCPR: 0.3522\n",
            "Epoch [69/100], Loss: 0.1142, AUCPR: 0.3655\n",
            "Epoch [70/100], Loss: 0.1154, AUCPR: 0.3682\n",
            "Epoch [71/100], Loss: 0.1152, AUCPR: 0.3674\n",
            "Epoch [72/100], Loss: 0.1144, AUCPR: 0.3637\n",
            "Epoch [73/100], Loss: 0.1126, AUCPR: 0.3784\n",
            "Epoch [74/100], Loss: 0.1141, AUCPR: 0.3700\n",
            "Epoch [75/100], Loss: 0.1137, AUCPR: 0.3659\n",
            "Epoch [76/100], Loss: 0.1129, AUCPR: 0.3764\n",
            "Epoch [77/100], Loss: 0.1134, AUCPR: 0.3734\n",
            "Epoch [78/100], Loss: 0.1140, AUCPR: 0.3626\n",
            "Epoch [79/100], Loss: 0.1138, AUCPR: 0.3611\n",
            "Epoch [80/100], Loss: 0.1125, AUCPR: 0.3856\n",
            "Epoch [81/100], Loss: 0.1140, AUCPR: 0.3657\n",
            "Epoch [82/100], Loss: 0.1133, AUCPR: 0.3705\n",
            "Epoch [83/100], Loss: 0.1136, AUCPR: 0.3786\n",
            "Epoch [84/100], Loss: 0.1119, AUCPR: 0.3867\n",
            "Epoch [85/100], Loss: 0.1126, AUCPR: 0.3731\n",
            "Epoch [86/100], Loss: 0.1120, AUCPR: 0.3804\n",
            "Epoch [87/100], Loss: 0.1125, AUCPR: 0.3805\n",
            "Epoch [88/100], Loss: 0.1128, AUCPR: 0.3870\n",
            "Epoch [89/100], Loss: 0.1133, AUCPR: 0.3791\n",
            "Epoch [90/100], Loss: 0.1123, AUCPR: 0.3784\n",
            "Epoch [91/100], Loss: 0.1116, AUCPR: 0.3848\n",
            "Epoch [92/100], Loss: 0.1119, AUCPR: 0.3831\n",
            "Epoch [93/100], Loss: 0.1117, AUCPR: 0.3891\n",
            "Epoch [94/100], Loss: 0.1114, AUCPR: 0.3928\n",
            "Epoch [95/100], Loss: 0.1105, AUCPR: 0.3935\n",
            "Epoch [96/100], Loss: 0.1103, AUCPR: 0.3981\n",
            "Epoch [97/100], Loss: 0.1113, AUCPR: 0.3837\n",
            "Epoch [98/100], Loss: 0.1108, AUCPR: 0.3941\n",
            "Epoch [99/100], Loss: 0.1112, AUCPR: 0.3869\n",
            "Epoch [100/100], Loss: 0.1116, AUCPR: 0.3951\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score\n",
        "import torch\n",
        "\n",
        "\n",
        "# TODO: repalce test_df and test_embeddings with train data!!!   \n",
        "def train_model(test_df, test_embeddings, model, optimizer, criterion, batch_size=256, epochs=25):\n",
        "    model.train()\n",
        "    \n",
        "    # Process data\n",
        "    input_list, labels = process_samples(test_df, test_embeddings)\n",
        "\n",
        "    # Create DataLoader\n",
        "    dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack(labels)  # Labels\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop for multiple epochs\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0  # Track loss for each epoch\n",
        "        \n",
        "        all_targets = []\n",
        "        all_outputs = []\n",
        "\n",
        "        for batch in dataloader:\n",
        "            input_emb, z_in, target = batch\n",
        "            target = target.unsqueeze(-1)  # Make target shape (batch_size, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_emb, z_in)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()  # Accumulate loss for epoch\n",
        "            \n",
        "            # Collect predictions & targets for AUCPR\n",
        "            all_outputs.append(output.detach().cpu())  # Move to CPU to avoid memory issues\n",
        "            all_targets.append(target.detach().cpu())\n",
        "\n",
        "        # Compute AUCPR at the end of the epoch\n",
        "        all_outputs = torch.cat(all_outputs).numpy()\n",
        "        all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "        aucpr = average_precision_score(all_targets, all_outputs)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}, AUCPR: {aucpr:.4f}\")\n",
        "\n",
        "# Model, optimizer, and loss function\n",
        "input_emb_dim = 182  # Assuming this based on test_df features\n",
        "model = Predictor(input_emb_dim=input_emb_dim)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "\n",
        "\n",
        "# TODO: repalce test_df and test_embeddings with train data!!!\n",
        "# Call training loop (assuming test_df and test_embeddings are available)\n",
        "train_model(test_df, test_embeddings, model, optimizer, criterion, epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: NOW WRITE A FUNC TO DO INFERENCE!!!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
